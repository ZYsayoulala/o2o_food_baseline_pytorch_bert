{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:38:53.075643Z",
     "start_time": "2020-05-22T15:38:50.521482Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0522 23:38:53.025776 16680 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I0522 23:38:53.066667 16680 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import collections\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from  transformers import  *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:38:53.125510Z",
     "start_time": "2020-05-22T15:38:53.077638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'comment'], dtype='object')\n",
      "(10000, 2)\n",
      "(2000, 3)\n",
      "label      0\n",
      "comment    0\n",
      "dtype: int64\n",
      "0    8489\n",
      "1    1511\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/train.csv\",sep='\\t')\n",
    "test = pd.read_csv(\"data/test_new.csv\",sep=',')\n",
    "sample = pd.read_csv(\"data/sample.csv\",sep=',')\n",
    "test['label']=0\n",
    "print(train.columns)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(train.isnull().sum())\n",
    "print(train.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data detact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:38:53.646118Z",
     "start_time": "2020-05-22T15:38:53.126508Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#content length\n",
    "train_data_content=train['comment'].str.len()\n",
    "test_data_content=test['comment'].str.len()\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,6))\n",
    "sns.distplot(train_data_content,ax=ax1,color='blue')\n",
    "sns.distplot(test_data_content,ax=ax2,color='green')\n",
    "ax2.set_title('Distribution for comment in test data')\n",
    "ax1.set_title('Distribution for comment in Training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:38:53.655094Z",
     "start_time": "2020-05-22T15:38:53.648113Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 150\n",
    "SEP_TOKEN_ID = 102\n",
    "DEVICE = 'cuda'\n",
    "Target_nums=['label']\n",
    "\n",
    "SEED=2019\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "BERT_MODEL_PATH='E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:38:53.672049Z",
     "start_time": "2020-05-22T15:38:53.658086Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#create DataSet\n",
    "class EmotionDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        token_ids, seg_ids = self.get_token_ids(row)\n",
    "        labels = self.get_label(row)\n",
    "        return token_ids,seg_ids,labels\n",
    "        \n",
    "    def select_tokens(self, tokens, max_num):\n",
    "        if len(tokens) <= max_num:\n",
    "            return tokens\n",
    "        else:\n",
    "            return tokens[:max_num]\n",
    "        \n",
    "    def get_seg_ids(self, ids):\n",
    "        seg_ids = torch.zeros_like(ids)\n",
    "        seg_idx = 0\n",
    "        for i, e in enumerate(ids):\n",
    "            seg_ids[i] = seg_idx\n",
    "            if e == SEP_TOKEN_ID:\n",
    "                seg_idx += 1\n",
    "        max_idx = torch.nonzero(seg_ids == seg_idx)\n",
    "        seg_ids[max_idx] = 0\n",
    "        return seg_ids\n",
    "\n",
    "    def get_label(self, row):\n",
    "        return torch.tensor(row[Target_nums].values.astype(np.float32))\n",
    "    \n",
    "    def get_token_ids(self,row):\n",
    "        tokens= ['[CLS]'] + self.select_tokens(self.tokenizer.tokenize(row.comment),MAX_LEN-2)+['[SEP]']\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "        #padding\n",
    "        if len(token_ids) < MAX_LEN:\n",
    "            token_ids += [0] * (MAX_LEN- len(token_ids))\n",
    "        #totensor\n",
    "        ids = torch.tensor(token_ids)[:MAX_LEN]\n",
    "        #segid\n",
    "        seg_ids = self.get_seg_ids(ids)\n",
    "        \n",
    "        return ids,seg_ids\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        token_ids = torch.stack([x[0] for x in batch])\n",
    "        seg_ids = torch.stack([x[1] for x in batch])\n",
    "        labels = torch.stack([x[2] for x in batch])\n",
    "        return token_ids, seg_ids, labels.squeeze()\n",
    "    \n",
    "\n",
    "#DataLoader\n",
    "def get_loader(df,batch_size=8,is_train=True):\n",
    "    ds_df = EmotionDataSet(df)\n",
    "    loader = torch.utils.data.DataLoader(ds_df, batch_size=batch_size, shuffle=is_train, num_workers=0, collate_fn=ds_df.collate_fn, drop_last=is_train)\n",
    "    loader.num = len(ds_df)\n",
    "    return loader\n",
    "\n",
    "def test_train_loader(train):\n",
    "    loader = get_loader(train,2)\n",
    "    for token_ids, seg_ids,labels in loader:\n",
    "        print(token_ids)\n",
    "        print(seg_ids)\n",
    "        print(labels)\n",
    "        break\n",
    "def test_test_loader(test):\n",
    "    if Target_nums not in list(test.columns):\n",
    "        test[Target_nums[0]]=0\n",
    "    loader = get_loader(test,2)\n",
    "    for token_ids, seg_ids,labels in loader:\n",
    "        print(token_ids)\n",
    "        print(seg_ids)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:38:53.716956Z",
     "start_time": "2020-05-22T15:38:53.673045Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0522 23:38:53.674043 16680 tokenization_utils.py:306] Model name 'E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/' is a path or url to a directory containing tokenizer files.\n",
      "I0522 23:38:53.675040 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/added_tokens.json. We won't load it.\n",
      "I0522 23:38:53.676038 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "I0522 23:38:53.677035 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/tokenizer_config.json. We won't load it.\n",
      "I0522 23:38:53.679030 16680 tokenization_utils.py:371] loading file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/vocab.txt\n",
      "I0522 23:38:53.680027 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:38:53.681025 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:38:53.682022 16680 tokenization_utils.py:371] loading file None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1343,  749, 1962, 1914, 3613,  749,  511,  511,  511, 2600,  860,\n",
      "         3341, 6432, 6820, 3221, 2523, 1962, 4638, 8024, 8024,  102,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 7226, 6940, 5520, 1400, 6929, 3340, 6662, 3178, 6804, 3221, 6498,\n",
      "         2145, 3341, 2421, 2094,  679, 1920, 1724, 2476, 3430, 6820, 1962, 1343,\n",
      "         2533, 3683, 6772, 3193,  677, 5831, 6820, 3221, 2923, 2571, 4638, 1920,\n",
      "         3519, 3221,  697,  702, 2399, 6768,  782, 5632, 2346, 2458, 4638, 2421,\n",
      "         1416, 1146, 7030, 1071, 2141, 2923, 6639, 4638, 7883, 5420,  966, 2533,\n",
      "         2972, 5773, 2418, 6421, 3221, 4215, 6814, 4197, 1400, 4924, 2544, 4171,\n",
      "         4171, 4294, 1166, 2075, 6382, 4696, 3683, 1912, 7481,  683, 7305, 1297,\n",
      "         7883, 5420, 4638, 6963, 1962, 1391, 4685, 3683,  722,  678, 3683, 5855,\n",
      "         4924, 3300, 6849, 5682, 1909, 2014, 1929, 4638, 1377,  809, 5440, 5991,\n",
      "         1217,  763, 2207, 7471, 3492,  672, 5489, 6996, 7481, 3300, 2521, 1217,\n",
      "         2487, 1922, 2397, 2697, 6230, 1008, 2397, 2863, 7481, 3766, 3300, 3739,\n",
      "         3723, 3146,  860, 2141, 2669, 6631,  966,  749,  102,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "test_train_loader(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:38:53.761836Z",
     "start_time": "2020-05-22T15:38:53.717925Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0522 23:38:53.719921 16680 tokenization_utils.py:306] Model name 'E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/' is a path or url to a directory containing tokenizer files.\n",
      "I0522 23:38:53.720918 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/added_tokens.json. We won't load it.\n",
      "I0522 23:38:53.721915 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "I0522 23:38:53.722913 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/tokenizer_config.json. We won't load it.\n",
      "I0522 23:38:53.723910 16680 tokenization_utils.py:371] loading file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/vocab.txt\n",
      "I0522 23:38:53.724907 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:38:53.725904 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:38:53.726902 16680 tokenization_utils.py:371] loading file None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2990, 1184, 5314, 2421, 1447, 6432, 1962,  749, 8024, 1168, 4157,\n",
      "         1114, 3198, 1391, 7649, 8024, 1591,  800,  812, 2828, 5831, 6963, 1114,\n",
      "         1906, 1962, 8024, 2769,  812,  738, 1114, 3198, 1168,  749, 8024, 5831,\n",
      "          738,  677, 7970,  749, 8024, 1762, 3517,  677, 8024, 1282,  702,  782,\n",
      "         8024, 5831, 4638,  819, 7030, 2523, 6639, 8024,  738, 2523, 1962, 1391,\n",
      "         8013, 6631,  966, 8013,  809, 1400, 3300, 3322,  833,  671, 2137, 6820,\n",
      "         3341, 8013,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 4692, 1168,  817, 3419,  912, 2139, 8024, 1730,  749, 6407, 6407,\n",
      "         1456, 6887, 8013, 2523, 6375, 2769, 2661, 1599, 8013, 5439, 3352, 3302,\n",
      "         1218, 2578, 2428, 2523, 1962, 8013, 2400, 3766, 3300, 1728,  711, 3221,\n",
      "         1730, 6579,  982, 2339, 1121, 3160, 8013, 4143, 7222, 4638, 3221,  702,\n",
      "         2207, 2358, 1520, 8013,  779,  812, 1914, 1914, 3118, 2898, 1158,  689,\n",
      "         1521, 8013,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "test_test_loader(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bulid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:38:55.276756Z",
     "start_time": "2020-05-22T15:38:53.762806Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0522 23:38:53.768790 16680 configuration_utils.py:148] loading configuration file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/config.json\n",
      "I0522 23:38:53.769788 16680 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0522 23:38:53.771782 16680 modeling_utils.py:334] loading weights file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# generator parameters: 102268417\n"
     ]
    }
   ],
   "source": [
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, n_classes=1):\n",
    "        super(EmotionModel, self).__init__()\n",
    "        self.model_name = 'EmotionModel'\n",
    "        self.bert_model = BertModel.from_pretrained(BERT_MODEL_PATH,cache_dir=None)\n",
    "        \n",
    "        self.fcc= nn.Sequential(nn.Linear(768, n_classes))\n",
    "        \n",
    "    \n",
    "    def forward(self,ids,seg_ids):\n",
    "        attention_mask = (ids > 0)\n",
    "        #last seq 是最后一层的输出\n",
    "        last_seq,pooled_output=self.bert_model(input_ids=ids, attention_mask=attention_mask)\n",
    "        out=self.fcc(last_seq[:,0,:]).sigmoid()\n",
    "        return out\n",
    "    \n",
    "\n",
    "def test_model():\n",
    "    x = torch.tensor([[1,2,3,4,5, 0, 0], [1,2,3,4,5, 0, 0]])\n",
    "        \n",
    "    seg_ids = torch.tensor([[1,0,0,0,0, 0, 0], [1,0,0,0,0, 0, 0]])\n",
    "    model = EmotionModel()\n",
    "    \n",
    "    y = model(x, seg_ids)\n",
    "    print(y)\n",
    "netG = EmotionModel()\n",
    "print('# generator parameters:', sum(param.numel() for param in netG.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:38:56.857543Z",
     "start_time": "2020-05-22T15:38:55.277755Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0522 23:38:55.278751 16680 configuration_utils.py:148] loading configuration file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/config.json\n",
      "I0522 23:38:55.280746 16680 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0522 23:38:55.281743 16680 modeling_utils.py:334] loading weights file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6288],\n",
      "        [0.6288]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T15:38:56.877490Z",
     "start_time": "2020-05-22T15:38:56.858541Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logs='- epoch - {0:2d} - train_loss - {1:.4f} train_score - {2:.3f} - val_loss - {3:.4f} - val_score - {4:.3f} - best_loss - {5:.3f}'\n",
    "\n",
    "#验证\n",
    "def metric_fn(p, t):\n",
    "    p=(p>0.5)*1\n",
    "    return f1_score(t,p)\n",
    "\n",
    "\n",
    "def validation_fn(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    y_pred, y_true, tloss = [], [], []\n",
    "    for ids,seg_ids,target in loader:\n",
    "        outputs = model(ids.cuda(DEVICE),seg_ids.cuda(DEVICE))\n",
    "        loss = loss_fn(outputs.squeeze(), target.cuda(DEVICE))\n",
    "        tloss.append(loss.item())\n",
    "        y_true.append(target.detach().cpu().numpy())\n",
    "        y_pred.append(outputs.detach().cpu().numpy())\n",
    "        \n",
    "    tloss = np.array(tloss).mean()\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    metric = metric_fn(y_pred, y_true)\n",
    "    return tloss, metric\n",
    "\n",
    "def predict_model_test(model,loader):\n",
    "    model.eval()\n",
    "    y_pred=[]\n",
    "    for ids,seg_ids,_ in loader:\n",
    "        outputs = model(ids.cuda(DEVICE),seg_ids.cuda(DEVICE))\n",
    "        y_pred.append(outputs.detach().cpu().numpy())\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model,train_loader,val_loader,early_stop_epochs=2,accumulation_steps=2,epochs=4,model_save_path='pytorch_Emotion_model.pkl'):  \n",
    "    \n",
    "    \n",
    "    no_improve_epochs=0\n",
    "    \n",
    "    ########优化器 学习率\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.8},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "            ]   \n",
    "    \n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "    \n",
    "    \n",
    "    train_len=len(train_loader)\n",
    "    \n",
    "    #loss function\n",
    "    loss_fn = nn.BCELoss().cuda(DEVICE)\n",
    "    best_vmetric=1.\n",
    "    \n",
    "    \n",
    "    logss=[]\n",
    "    \n",
    "    \n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        y_pred, y_true = [], []\n",
    "        start_time = time.time\n",
    "        tloss = []\n",
    "        model.train()\n",
    "        bar = tqdm_notebook(train_loader)\n",
    "        for i,(ids, seg_ids,labels) in enumerate(bar):\n",
    "            outputs = model(ids.cuda(DEVICE),seg_ids.cuda(DEVICE))\n",
    "#             print(labels)\n",
    "            loss = loss_fn(outputs.squeeze(), labels.cuda(DEVICE))\n",
    "            tloss.append(loss.item())\n",
    "            loss.backward()\n",
    "            #梯度累计\n",
    "            if (i+1) % accumulation_steps == 0 or (i+1)==train_len:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            y_true.append(labels.detach().cpu().numpy())\n",
    "            y_pred.append(outputs.detach().cpu().numpy())\n",
    "            y_t = np.concatenate(y_pred)\n",
    "            y_p = np.concatenate(y_true)\n",
    "            metric = metric_fn(y_t,y_p)\n",
    "            bar.set_postfix(loss=np.array(tloss).mean(),score=metric)\n",
    "        \n",
    "        tloss = np.array(tloss).mean()\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        y_true = np.concatenate(y_true)\n",
    "        tmetric = metric_fn(y_pred, y_true)\n",
    "        vloss, vmetric = validation_fn(model, val_loader, loss_fn)\n",
    "        \n",
    "        #test\n",
    "        \n",
    "        p=logs.format(epoch,tloss,tmetric,vloss,vmetric,best_vmetric)\n",
    "        logss.append(p)\n",
    "        print(p)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #save best model\n",
    "        if vloss<=best_vmetric:\n",
    "            torch.save(model.state_dict(),model_save_path)\n",
    "            best_vmetric=vloss\n",
    "            no_improve_epochs=0\n",
    "            print('improve save model!!!')\n",
    "        else:\n",
    "            no_improve_epochs+=1\n",
    "        ###for eary stop\n",
    "        if no_improve_epochs==early_stop_epochs:\n",
    "            print('no improve score !!! stop train !!!')\n",
    "            break\n",
    "        \n",
    "    return logss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T16:00:56.501118Z",
     "start_time": "2020-05-22T15:38:56.878487Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0522 23:38:56.884471 16680 configuration_utils.py:148] loading configuration file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/config.json\n",
      "I0522 23:38:56.886467 16680 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0522 23:38:56.887464 16680 modeling_utils.py:334] loading weights file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin\n",
      "I0522 23:39:00.974544 16680 tokenization_utils.py:306] Model name 'E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/' is a path or url to a directory containing tokenizer files.\n",
      "I0522 23:39:00.976538 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/added_tokens.json. We won't load it.\n",
      "I0522 23:39:00.976538 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "I0522 23:39:00.977536 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/tokenizer_config.json. We won't load it.\n",
      "I0522 23:39:00.978535 16680 tokenization_utils.py:371] loading file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/vocab.txt\n",
      "I0522 23:39:00.979532 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:39:00.980529 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:39:00.980529 16680 tokenization_utils.py:371] loading file None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----------------------start\n",
      "(8000, 2)\n",
      "(2000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0522 23:39:01.005461 16680 tokenization_utils.py:306] Model name 'E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/' is a path or url to a directory containing tokenizer files.\n",
      "I0522 23:39:01.006459 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/added_tokens.json. We won't load it.\n",
      "I0522 23:39:01.007456 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "I0522 23:39:01.007456 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/tokenizer_config.json. We won't load it.\n",
      "I0522 23:39:01.008454 16680 tokenization_utils.py:371] loading file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/vocab.txt\n",
      "I0522 23:39:01.009451 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:39:01.009451 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:39:01.010448 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:39:01.026406 16680 tokenization_utils.py:306] Model name 'E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/' is a path or url to a directory containing tokenizer files.\n",
      "I0522 23:39:01.028400 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/added_tokens.json. We won't load it.\n",
      "I0522 23:39:01.028400 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "I0522 23:39:01.029398 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/tokenizer_config.json. We won't load it.\n",
      "I0522 23:39:01.030395 16680 tokenization_utils.py:371] loading file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/vocab.txt\n",
      "I0522 23:39:01.031392 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:39:01.031392 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:39:01.032390 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:39:01.048347 16680 configuration_utils.py:148] loading configuration file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/config.json\n",
      "I0522 23:39:01.049344 16680 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0522 23:39:01.050341 16680 modeling_utils.py:334] loading weights file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff4aa372b1547f8b6b85e73dbf4f6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- epoch -  1 - train_loss - 0.1025 train_score - 0.866 - val_loss - 0.1248 - val_score - 0.890 - best_loss - 1.000\n",
      "improve save model!!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bdffe5ecec34010809e65bd39d01b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- epoch -  2 - train_loss - 0.0507 train_score - 0.937 - val_loss - 0.0858 - val_score - 0.912 - best_loss - 0.125\n",
      "improve save model!!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e2cd9d98e24502a0afdcb5ea06358a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- epoch -  3 - train_loss - 0.0299 train_score - 0.966 - val_loss - 0.1035 - val_score - 0.916 - best_loss - 0.086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350a53e96d1244f3a6d92b6394b69256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- epoch -  4 - train_loss - 0.0149 train_score - 0.983 - val_loss - 0.1219 - val_score - 0.912 - best_loss - 0.086\n",
      "no improve score !!! stop train !!!\n",
      "1 -----------------------end\n",
      "2 ----------------------start\n",
      "(8000, 2)\n",
      "(2000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0522 23:58:32.044326 16680 tokenization_utils.py:306] Model name 'E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/' is a path or url to a directory containing tokenizer files.\n",
      "I0522 23:58:32.045322 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/added_tokens.json. We won't load it.\n",
      "I0522 23:58:32.045322 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "I0522 23:58:32.046320 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/tokenizer_config.json. We won't load it.\n",
      "I0522 23:58:32.047317 16680 tokenization_utils.py:371] loading file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/vocab.txt\n",
      "I0522 23:58:32.047317 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:58:32.048315 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:58:32.048315 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:58:32.068262 16680 tokenization_utils.py:306] Model name 'E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/' is a path or url to a directory containing tokenizer files.\n",
      "I0522 23:58:32.069260 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/added_tokens.json. We won't load it.\n",
      "I0522 23:58:32.070257 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "I0522 23:58:32.071254 16680 tokenization_utils.py:335] Didn't find file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/tokenizer_config.json. We won't load it.\n",
      "I0522 23:58:32.073248 16680 tokenization_utils.py:371] loading file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/vocab.txt\n",
      "I0522 23:58:32.074246 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:58:32.075243 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:58:32.075243 16680 tokenization_utils.py:371] loading file None\n",
      "I0522 23:58:32.092198 16680 configuration_utils.py:148] loading configuration file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/config.json\n",
      "I0522 23:58:32.093195 16680 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0522 23:58:32.094222 16680 modeling_utils.py:334] loading weights file E:/code/pre_model/pytorch_bert/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2d0d34fbfe4a43a30e820625e921c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-26118d181157>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     logss=train_model(model,train_loader,val_loader,early_stop_epochs=early_stop_epochs,\n\u001b[0;32m     28\u001b[0m                       \u001b[0maccumulation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccumulation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                       model_save_path=model_save_path)\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict_model_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-e229b0aab2be>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, early_stop_epochs, accumulation_steps, epochs, model_save_path)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseg_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;31m#             print(labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\professional_software\\anaconda\\lib\\site-packages\\tqdm\\_tqdm_notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\professional_software\\anaconda\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1058\u001b[0m                 \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1060\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1061\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\professional_software\\anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\professional_software\\anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\professional_software\\anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-8fc7f692f729>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_token_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseg_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-8fc7f692f729>\u001b[0m in \u001b[0;36mget_token_ids\u001b[1;34m(self, row)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m#segid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mseg_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_seg_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseg_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-8fc7f692f729>\u001b[0m in \u001b[0;36mget_seg_ids\u001b[1;34m(self, ids)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mseg_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseg_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mSEP_TOKEN_ID\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m                 \u001b[0mseg_idx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mmax_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_ids\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mseg_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "FOLD=5\n",
    "BATXH_SIZE=8\n",
    "EPOCH=10\n",
    "accumulation_steps=2\n",
    "early_stop_epochs=2\n",
    "netG = EmotionModel().cuda(DEVICE)\n",
    "\n",
    "\n",
    "test_preds=test[['id']]\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=FOLD, shuffle=True,random_state=2019)\n",
    "test_loader=get_loader(test,BATXH_SIZE//2,False)\n",
    "sum_preds=np.zeros((len(test),1))\n",
    "for i,(train_index , test_index) in enumerate(kf.split(train,train)):\n",
    "    \n",
    "    print(i+1,'----------------------start')\n",
    "    tra=train.iloc[train_index,:]\n",
    "    val=train.iloc[test_index,:]\n",
    "    \n",
    "    print(tra.shape)\n",
    "    print(val.shape)\n",
    "    train_loader=get_loader(tra,BATXH_SIZE,True)\n",
    "    val_loader=get_loader(val,BATXH_SIZE//2,False)\n",
    "    \n",
    "    model = EmotionModel().cuda(DEVICE)\n",
    "    model_save_path='model_all_QA_{}.pkl'.format(i+1)\n",
    "    logss=train_model(model,train_loader,val_loader,early_stop_epochs=early_stop_epochs,\n",
    "                      accumulation_steps=accumulation_steps,epochs=EPOCH,\n",
    "                      model_save_path=model_save_path)\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    preds=predict_model_test(model,test_loader)\n",
    "    test_preds.loc[:,'fold_label_{}'.format(i+1)]=(preds>0.5)*1\n",
    "    sum_preds+=preds\n",
    "    print(i+1,'-----------------------end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T16:00:58.186438Z",
     "start_time": "2020-05-22T16:00:58.156518Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['fold_label_2'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c96be4c90a06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFOLD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0msub_i\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'fold_label_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0msub_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msub_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sub{}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msum_preds\u001b[0m\u001b[1;33m/=\u001b[0m\u001b[0mFOLD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\professional_software\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[1;32m-> 2934\u001b[1;33m                                                    raise_missing=True)\n\u001b[0m\u001b[0;32m   2935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\professional_software\\anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[0;32m   1353\u001b[0m                           raise_missing}\n\u001b[1;32m-> 1354\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1355\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\professional_software\\anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[0;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1161\u001b[1;33m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[0;32m   1162\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\professional_software\\anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1250\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'loc'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1252\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} not in index\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnot_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m             \u001b[1;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['fold_label_2'] not in index\""
     ]
    }
   ],
   "source": [
    "for i in range(FOLD):\n",
    "    sub_i=test_preds[['id','fold_label_{}'.format(i+1)]]\n",
    "    sub_i.columns=['id','label']\n",
    "    sub_i.to_csv('sub{}.csv'.format(i+1),index=False)\n",
    "sum_preds/=FOLD\n",
    "test_preds['label']=(sum_preds>0.5)*1\n",
    "test_preds[['id','label']].to_csv('sub_mean',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
